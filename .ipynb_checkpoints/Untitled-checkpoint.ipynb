{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step:1\n",
    "### First I will use the file make_df.py to make my dataframe from data stored in mongo and clean it\n",
    "#### If you have already made your df, skip to step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "# Make sure we have nltk packages\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import gensim\n",
    "from gensim.utils import lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df(db_name):\n",
    "    \"\"\"\n",
    "    Function to get create Pandas DataFrame from JSON files in Mongo database.\n",
    "    Following are the steps we take:\n",
    "\n",
    "    1. Get collections from Mongo\n",
    "    2. Loop through collections and find which site it refers to\n",
    "    3. Save variables from JSON file to Pandas DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    db_name: Name of Mongo database\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    df: DataFrame from Mongo (not clean)\n",
    "    \"\"\"\n",
    "    url_names = ['wsj', 'cnn', 'abc', 'fox', 'nyt', 'reuters', 'wapo', 'huffpo', 'esquire', 'rollingstone', 'cbs', '538', 'washtimes']\n",
    "    client = MongoClient()\n",
    "    db = client[db_name]\n",
    "\n",
    "    collection_names = db.collection_names()\n",
    "    my_collection_names = [name for name in collection_names]\n",
    "\n",
    "    df = pd.DataFrame(columns=['article_text', 'author', 'date_published', 'headline', 'url', 'source'])\n",
    "    for collection_name in my_collection_names:\n",
    "        if collection_name != 'system.indexes':\n",
    "            site = [name for name in url_names if collection_name.startswith(name)]\n",
    "            if len(site) != 0:\n",
    "                site = site[0]\n",
    "                print('Working on '+collection_name)\n",
    "                for article in db[collection_name].find():\n",
    "                    # remove article that just have videos\n",
    "                    # remove powerpost articles from wapo becaue they are 4000+ words\n",
    "                    if 'video' not in article['url'] and 'powerpost' not in article['url']:\n",
    "                        try:\n",
    "                            url = article['url']\n",
    "                            source = site\n",
    "                            headline = article['headline']\n",
    "                            date_published = article['date_published']\n",
    "                            # If date is missing then use the collection name date\n",
    "                            if type(date_published) == float:\n",
    "                                date = collection_name.split('_')[1]\n",
    "                                date_published = datetime.datetime.strptime(date, '%Y%m%d')\n",
    "                            author = article['author']\n",
    "                            article_text = article['article_text']\n",
    "                            # df.append(pd.Series([article_text, author, date_published, headline, url, processed_text, source]), ignore_index=True)\n",
    "                            df.loc[-1] = [article_text, author, date_published, headline, url, source]  # adding a row\n",
    "                            df.index = df.index + 1  # shifting index\n",
    "                            df = df.sort()  # sorting by index\n",
    "                        except:\n",
    "                            print('Problem with article in '+site)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_str(x):\n",
    "    '''Checks if string or unidecode'''\n",
    "    if isinstance(x, str):\n",
    "        return unidecode(x)\n",
    "    else:\n",
    "        return str(x)\n",
    "\n",
    "def fix_cnn(df):\n",
    "    \"\"\"\n",
    "    Function to fix CNN articles because the library Newspaper didn't scrape these correctly,\n",
    "    so I use BeautifulSoup to fix them\n",
    "    \"\"\"\n",
    "    for i in range(df.shape[0]):\n",
    "        if df['source'][i] == 'cnn':\n",
    "            url = df['url'][i]\n",
    "            try:\n",
    "                result = requests.get(url)\n",
    "                soup = BeautifulSoup(result.content, 'html.parser')\n",
    "\n",
    "                tag1 = soup.find('div', attrs={'class': 'el__leafmedia el__leafmedia--sourced-paragraph'}).text\n",
    "                tag2 = soup.find_all('div', attrs={'class': 'zn-body__paragraph speakable'})\n",
    "                tag3 = soup.find_all('div', attrs={'class': 'zn-body__paragraph'})\n",
    "                new_article_text = tag1+' /n '+parse_str(' \\n '.join([line.text for line in tag2]))+parse_str(' \\n '.join([line.text for line in tag3]))\n",
    "                df['article_text'][i] = new_article_text\n",
    "            except:\n",
    "                pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    \"\"\"\n",
    "    Function to clean dataframe. Following are the steps we take:\n",
    "\n",
    "    1. Remove unwanted sites (because they weren't strictly political)\n",
    "    2. Remove unwanted articles from sites (Some articles were just photos or transcripts)\n",
    "    3. Remove null values from article_text\n",
    "    \"\"\"\n",
    "    # Remove duplicates by url\n",
    "    df = df.drop_duplicates(subset='url')\n",
    "    # Below I get rid of large articles that could throw off my algorithms\n",
    "    # Remove two article types that were very long\n",
    "    # any url with speech was just a transcript of speeches\n",
    "    df = df[(df['source'] != 'ap') & (df['source'] != 'economist') & (df['source'] != 'vox') & (df['source'] != 'time') & (df['source'] != 'slate')]\n",
    "\n",
    "    df[df['source'] == 'wapo'] = df[(df['source'] == 'wapo') & (df['url'].str.contains('powerpost') == False) & (df['url'].str.contains('-speech-') == False)]\n",
    "    df[df['source'] == 'economist'] = df[(df['source'] == 'economist') & (df['url'].str.contains('transcript') == False)]\n",
    "    df[df['source'] == 'fox'] = df[(df['source'] == 'fox') & (df['article_text'].str.contains('Want FOX News Halftime Report in your inbox every day?') == False)]\n",
    "    df[df['source'] == 'esquire'] = df[(df['source'] == 'esquire') & (df['url'].str.contains('-gallery-') == False)]\n",
    "\n",
    "    # Can't have null values in text\n",
    "    df = df[pd.notnull(df['article_text'])]\n",
    "    df = df.dropna(how='all')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_texts(text):\n",
    "    '''Uses gensim to get preprocessed text'''\n",
    "    for line in text:\n",
    "        yield gensim.utils.simple_preprocess(line, deacc=True, min_len=3)\n",
    "\n",
    "\n",
    "\n",
    "def process_article(headline_text, article_text, bigram, trigram):\n",
    "    \"\"\"\n",
    "    Function to process texts. Following are the steps we take:\n",
    "\n",
    "    1. Seperate quotes and tweets.\n",
    "    2. Stopword and unwanted word Removal.\n",
    "    3. Bigram, trigram, quadgram creation.\n",
    "    4. Lemmatization (not stem since stemming can reduce the interpretability).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    headline_text, article_text: string.\n",
    "    bigram, trigram: Already trained bigram and trigram from gensim\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    headline, article, quotes, tweets: Pre-processed tokenized texts.\n",
    "    \"\"\"\n",
    "#     article_text = [[word for word in line if word not in stops] for line in article_text]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    article_quotes1 = ' '.join(re.findall('“.*?”', article_text))\n",
    "    article_quotes2 = ' '.join(re.findall('\".*?\"', article_text))\n",
    "    headline_quotes1 = ' '.join(re.findall('“.*?”', headline_text))\n",
    "    headline_quotes2 = ' '.join(re.findall('\".*?\"', headline_text))\n",
    "    quotes = article_quotes1 + article_quotes2 + headline_quotes1 + headline_quotes2\n",
    "\n",
    "    tweets = ' '.join(re.findall('\\n\\n.*?@', article_text))+' '+' '.join(re.findall('\\n\\n@.*?@', article_text))\n",
    "\n",
    "    # remove tweets\n",
    "    article_text = re.sub('\\n\\n.*?@', '', article_text)\n",
    "    article_text = re.sub('\\n\\n@.*?@', '', article_text)\n",
    "    headline_text = re.sub('\\n\\n.*?@', '', headline_text)\n",
    "    headline_text = re.sub('\\n\\n@.*?@', '', headline_text)\n",
    "\n",
    "    article_text = ' '.join([word for word in article_text.split(' ') if not word.startswith('(@') and not word.startswith('http')])\n",
    "\n",
    "    # remove quotes\n",
    "    article_text = re.sub('“.*?”', '', article_text)\n",
    "    article_text = re.sub('\".*?\"', '', article_text)\n",
    "\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    sw = set(stopwords.words('english'))\n",
    "    wordnet = WordNetLemmatizer()\n",
    "\n",
    "    article_text = article_text.lower()\n",
    "    headline_text = headline_text.lower()\n",
    "    quotes = quotes.lower()\n",
    "    tweets = tweets.lower()\n",
    "\n",
    "    article_text_tokens = tokenizer.tokenize(article_text)\n",
    "    headline_text_tokens = tokenizer.tokenize(headline_text)\n",
    "    quotes_tokens = tokenizer.tokenize(quotes)\n",
    "    tweets_tokens = tokenizer.tokenize(tweets)\n",
    "\n",
    "    # remove stop words and unwanted words\n",
    "    words_to_remove = ['http', 'com', '_', '__', '___', 'mr']\n",
    "    article_text_stopped_tokens = [i for i in article_text_tokens if i not in sw and i not in words_to_remove]\n",
    "    headline_text_stopped_tokens = [i for i in headline_text_tokens if i not in sw and i not in words_to_remove]\n",
    "    quotes_stopped_tokens = [i for i in quotes_tokens if not i in sw and i not in words_to_remove]\n",
    "    tweets_stopped_tokens = [i for i in tweets_tokens if not i in sw and i not in words_to_remove]\n",
    "\n",
    "    # Create bigrams\n",
    "    article_text_stopped_tokens = bigram[article_text_stopped_tokens]\n",
    "    headline_text_stopped_tokens = bigram[headline_text_stopped_tokens]\n",
    "    quotes_stopped_tokens = bigram[quotes_stopped_tokens]\n",
    "    tweets_stopped_tokens = bigram[tweets_stopped_tokens]\n",
    "\n",
    "    # Create trigrams (and quadgrams)\n",
    "    article_text_stopped_tokens = trigram[bigram[article_text_stopped_tokens]]\n",
    "    headline_text_stopped_tokens = trigram[bigram[headline_text_stopped_tokens]]\n",
    "    quotes_stopped_tokens = trigram[bigram[quotes_stopped_tokens]]\n",
    "    tweets_stopped_tokens = trigram[bigram[tweets_stopped_tokens]]\n",
    "\n",
    "    # stem token\n",
    "    article_text = [wordnet.lemmatize(i) for i in article_text_stopped_tokens]\n",
    "    headline_text = [wordnet.lemmatize(i) for i in headline_text_stopped_tokens]\n",
    "    quotes = [wordnet.lemmatize(i) for i in quotes_stopped_tokens]\n",
    "    tweets = [wordnet.lemmatize(i) for i in tweets_stopped_tokens]\n",
    "\n",
    "    return headline_text, article_text, quotes, tweets\n",
    "\n",
    "def process_articles(df):\n",
    "    '''Uses process_article to process all articles in a dataframe'''\n",
    "    articles = df['article_text'].values.tolist()\n",
    "\n",
    "    # Used to train bigrams and trigrams\n",
    "    train_texts = list(build_texts(articles))\n",
    "\n",
    "    bigram = gensim.models.Phrases(train_texts)  # for bigram collocation detection\n",
    "    trigram = gensim.models.Phrases(bigram[train_texts])  # for trigram collocation detection\n",
    "\n",
    "    # topic_texts used to create topics (includes headlines, articles, tweets and quotes)\n",
    "    topic_texts = []\n",
    "    # sentiment_texts used to calculate sentiment (only includes headlines and articles)\n",
    "    sentiment_texts = []\n",
    "    quote_texts = []\n",
    "    tweet_texts = []\n",
    "    for headline, article in zip(df['headline'].tolist(), df['article_text'].tolist()):\n",
    "        all_texts = process_article(headline, article, bigram, trigram)\n",
    "        topic_texts.append(all_texts[0] + all_texts[1] + all_texts[2] + all_texts[3])\n",
    "        sentiment_texts.append(all_texts[0] + all_texts[1])\n",
    "        quote_texts.append(all_texts[2])\n",
    "        tweet_texts.append(all_texts[3])\n",
    "\n",
    "    df['topic_texts'] = [' '.join(text) for text in topic_texts]\n",
    "    df['sentiment_texts'] = [' '.join(text) for text in sentiment_texts]\n",
    "    df['quote_texts'] = [' '.join(text) for text in quote_texts]\n",
    "    df['tweet_texts'] = [' '.join(text) for text in tweet_texts]\n",
    "\n",
    "    return topic_texts, sentiment_texts, quote_texts, tweet_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = get_df('rss_feeds_new')\n",
    "df = fix_cnn(df)\n",
    "# df = fix_huffpo(df)\n",
    "df = clean_df(df)\n",
    "df = df[pd.notnull(df['article_text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will now get tones for each article from IBM's Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def watson_tone_analyzer(df, have_tones=True):\n",
    "    new_df = None\n",
    "    if have_tones:\n",
    "        new_df = df[df['tones'].isnull()]\n",
    "    else:\n",
    "        new_df = df\n",
    "    tones = df['tones']\n",
    "    for i in range(len(new_df.shape[0])):\n",
    "        json_response_sentiment = tone_analyzer.tone(text=' '.join(ast.literal_eval(new_df['sentiment_texts'][i])), sentences=False)\n",
    "        tones[i] = parse_toneanalyzer_response(json_response_sentiment)\n",
    "\n",
    "    df['tones'] = tones\n",
    "    return tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = watson_tone_analyzer(df, have_tones=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can skip all the steps ahead if you have already made the df by just loading it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/rss_feeds_with_tones.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import ast\n",
    "from datetime import datetime, date\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "import warnings\n",
    "# Gensim gives annoying warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import LdaModel, HdpModel\n",
    "from gensim.corpora import Dictionary\n",
    "import ast\n",
    "\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coverage_by_site_by_topic(df_all, topic_dict):\n",
    "    colors = {'538': '#ff0000', 'abc': '#00ff00', 'cbs': '#0000ff', 'cnn': '#006400', 'esquire': '#000000', 'fox': '#6419c8', 'huffpo': '#ffff00', 'nyt': '#00ffff', 'reuters': '#ff00ff', 'rollingstone': '#808080', 'wapo': '#000080', 'washtimes': '#f0e68c'}\n",
    "    df_count = df_all.groupby('source')['date_published'].count()\n",
    "    figs = []\n",
    "#     for topic in range(len(topic_dict)):\n",
    "    for topic in range(1):\n",
    "        dates = topic_dict[topic]['date_published']\n",
    "        sources = topic_dict[topic]['source']\n",
    "        df = pd.DataFrame({'date_published' : pd.Series(dates), 'source': pd.Series(sources)})\n",
    "        df = df[df['date_published'] > date(2017,5,18)]\n",
    "        y_max = (df['date_published'].dt.date.value_counts()/df_count.values.max()).max()\n",
    "        idx_dates = pd.date_range(df['date_published'].dt.date.min().isoformat(), df['date_published'].dt.date.max().isoformat())\n",
    "\n",
    "        fig = plt.figure(figsize=(20,24), dpi=300)\n",
    "        ax_all = fig.add_subplot(111)\n",
    "        # ax_all.set_xlabel('date')\n",
    "        ax_all.set_ylabel('Coverage').set_fontsize(20)\n",
    "        ax_all.set_title('Coverage of Topic '+str(topic)+' by Site').set_fontsize(20)\n",
    "        for k,v in ax_all.spines.items():\n",
    "            v.set_visible(False)\n",
    "        ax_all.set_xticks([])\n",
    "        ax_all.set_yticks([])\n",
    "        axes = []\n",
    "        y_lim = []\n",
    "        for i,source in enumerate(np.unique(df['source'])):\n",
    "            ax = fig.add_subplot(12,1,i+1)\n",
    "            new_df = df[df['source'] == source]\n",
    "\n",
    "            new_df = new_df['date_published'].dt.date.value_counts()\n",
    "            new_df.sort_index(inplace=True)\n",
    "            new_df = new_df.reindex(idx_dates, fill_value=0)\n",
    "            new_df = new_df/df_count['cnn']\n",
    "            x = new_df.index\n",
    "            y = new_df.values\n",
    "            ax.plot(x, y, label=source, color=colors[source], linewidth=3.0)\n",
    "            ax.legend(loc='upper left', prop={'size':10})\n",
    "            for k,v in ax.spines.items():\n",
    "                v.set_visible(False)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            axes.append(ax)\n",
    "            y_lim.append(new_df.values.max())\n",
    "        for ax in axes:\n",
    "            ax.set_ylim(0,max(y_lim))\n",
    "            \n",
    "        axes[-1].set_xticks(idx_dates).set_fontsize(20)\n",
    "        xfmt = mdates.DateFormatter('%m-%d')\n",
    "        axes[-1].xaxis.set_major_formatter(xfmt).set_fontsize(20)\n",
    "        fig.autofmt_xdate()\n",
    "        plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "        figs.append(fig)\n",
    "    return figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/rss_with_tones_in_df_fixed_time.csv')\n",
    "with open('pickles/lda_model.pkl', 'rb') as f:\n",
    "    lda_model = pickle.load(f)\n",
    "with open('pickles/topic_dict.pkl', 'rb') as f:\n",
    "    topic_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coverage_figs = coverage_by_site_by_topic(df,topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "coverage_figs[0].savefig('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
